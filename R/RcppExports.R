# Generated by using Rcpp::compileAttributes() -> do not edit by hand
# Generator token: 10BE3573-1514-4C36-9D1C-5A225CD40393

#' CG class.
#' @name CG
#' @description
#' The function includes initialization of the class(CG()), calculate the loss of beta_k,two ways to determine the step(alpha),the solving function of minimize loss by iteration,and complete solution of the function(getResult())
NULL

#'Compute the optimal estimates of beta by minimize the loss function with l2 penalty through CG method
#'
#'@param X the independent variable matrix
#'@param y the dependent variable vector
#'@param lambda the coeffient of l2 regularization
#'@t the coeffient of the Dai-Liao's conjugacy conditions when solving the co_factor of search direction
#'@eps the threshold of gradient to judge whether converging
#'@max_iteration the maximum number of iterations
#'@return the list contains the estimated value of beta,the matrix of the beta under every iteration,the iterations number,the final loss
#'@examples
#'seef_num = 123
#'n = 100
#'p = 2
#'simudata = getdata(seed_num,n,p)
#'result = RidgeRegression_CG(simudata$X,simudata$y,lambda = 0.1,max_iteration = 1000)
#'result.betaHat
RidgeRegression_CG <- function(X, y, lambda, t = 0.1, eps = 1e-5, max_iteration = 1000L) {
    .Call(`_RRopt_RidgeRegression_CG`, X, y, lambda, t, eps, max_iteration)
}

#' ADMM class.
#' @name ADMM
#' @description
#' The attributes include regression data(dataX, dataY, lambda, rho), iteration parameters(max_iteration, eps_abs, eps_rel), variables to be solved(x, z, u), etc;
#' The function includes initialization of the class(ADMM()), determination of convergence(ifConvergent()), updates of each solving variable(UpdateX(), UpdateZ(), UpdateU()), and complete solution of the function(getResult()).
NULL

#' Solve ridge regression using ADMM.
#' 
#' @param X the independent variable X (size n×p)
#' @param y the response variable y (size n×1)
#' @param lambda the penalty factor (size 1×1)
#' @param rho the lagrange multiplier (size 1×1)
#' @param max_iter the maximum number of iterations
#' @param eps_abs the absolute convergence threshold
#' @param eps_rel the relative convergence threshold
#' @return Estimated value of beta(last and historical), iterations and loss
RidgeRegression_ADMM <- function(X, y, lambda, rho = 0.1, max_iter = 1000L, eps_abs = 1e-5, eps_rel = 1e-5) {
    .Call(`_RRopt_RidgeRegression_ADMM`, X, y, lambda, rho, max_iter, eps_abs, eps_rel)
}

#' class L_BFGS: using L-BFGS algorithm to solve the Optimization Problems
#'               where the target_objection is ridge regression.
#' 
#' @name L_BFGS
#' @description
#' The function includes initialization of the class(L-BFGS()), 
#' the ridge regression loss function(ridge_objective()),
#' function to monitor the progress of the optimization process(monitorProgress()), 
#' the solving function of minimize loss by iteration(monitorProgress()),
#' and complete solution of the function by my_lbfgs::lbfgs_optimize(getResult())
NULL

#' Solve ridge regression using L-BFGS.
#' 
#' @param X the independent variable X (size n×p)
#' @param y the response variable y (size n×1)
#' @param lambda the penalty factor (size 1×1)
#' @param men_size the number of corrections to approximate the inverse hessian matrix
#' @param max_iterations the maximum number of iterations
#' @param g_epsilon the epsilon for grad convergence test
#' @param past the distance for delta-based convergence test
#' @param delta for convergence test
#' @return Estimated value of targetValues, beta(last and historical), iterations and loss
RidgeRegression_LBFGS <- function(X, y, lambda, mem_size = 8L, max_iterations = 64L, g_epsilon = 1.0e-5, past = 3L, delta = 1.0e-6) {
    .Call(`_RRopt_RidgeRegression_LBFGS`, X, y, lambda, mem_size, max_iterations, g_epsilon, past, delta)
}

